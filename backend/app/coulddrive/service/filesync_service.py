#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from __future__ import annotations

import json
import logging
import os
import re
import time
from collections import defaultdict
from datetime import datetime
from pathlib import PurePosixPath
from typing import Any, Dict, List, Optional, Pattern, Set, Tuple, Union

from sqlalchemy.ext.asyncio import AsyncSession

from backend.app.coulddrive.model.filesync import SyncConfig, SyncTask, SyncTaskItem
from backend.app.coulddrive.schema.enum import DriveType, ItemType, MatchMode, MatchTarget, RecursionSpeed, SyncMethod
from backend.app.coulddrive.schema.file import (
    BaseFileInfo,
    DiskTargetDefinition,
    ExclusionRuleDefinition,
    GetCompareDetail,
    RenameRuleDefinition,
    ShareSourceDefinition,
    ListFilesParam,
    ListShareFilesParam,
    TransferParam,
    RemoveParam,
    MkdirParam,
)
from backend.app.coulddrive.schema.filesync import GetSyncConfigDetail
from backend.app.coulddrive.schema.user import GetDriveAccountDetail
from backend.app.coulddrive.service.yp_service import get_drive_manager

logger = logging.getLogger(__name__)

class FileSyncService:
    """æ–‡ä»¶åŒæ­¥æœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ–‡ä»¶åŒæ­¥æœåŠ¡"""
        # ç§»é™¤é‡å¤çš„å®¢æˆ·ç«¯ç¼“å­˜ï¼Œç›´æ¥ä½¿ç”¨å…¨å±€ç®¡ç†å™¨
        pass
    
    def _parse_sync_method(self, method_str: str) -> str:
        """è§£æåŒæ­¥æ–¹å¼
        
        Args:
            method_str: åŒæ­¥æ–¹å¼å­—ç¬¦ä¸²
            
        Returns:
            str: æ ‡å‡†åŒ–çš„åŒæ­¥æ–¹å¼
        """
        # å°è¯•åŒ¹é…æšä¸¾å€¼
        method_lower = method_str.lower() if method_str else ""
        
        if method_lower == SyncMethod.INCREMENTAL.value:
            return SyncMethod.INCREMENTAL.value
        elif method_lower == SyncMethod.FULL.value:
            return SyncMethod.FULL.value
        elif method_lower == SyncMethod.OVERWRITE.value:
            return SyncMethod.OVERWRITE.value
        else:
            # é»˜è®¤ä½¿ç”¨å¢é‡åŒæ­¥
            logger.warning(f"æœªçŸ¥çš„åŒæ­¥æ–¹å¼: {method_str}ï¼Œä½¿ç”¨é»˜è®¤å¢é‡åŒæ­¥")
            return SyncMethod.INCREMENTAL.value

    def _parse_recursion_speed(self, speed_value: int) -> RecursionSpeed:
        """è§£æé€’å½’é€Ÿåº¦
        
        Args:
            speed_value: é€Ÿåº¦å€¼ï¼ˆ0-2ï¼‰
            
        Returns:
            RecursionSpeed: é€’å½’é€Ÿåº¦æšä¸¾
        """
        if speed_value == 1:
            return RecursionSpeed.SLOW
        elif speed_value == 2:
            return RecursionSpeed.FAST
        else:
            # é»˜è®¤ä½¿ç”¨æ­£å¸¸é€Ÿåº¦
            return RecursionSpeed.NORMAL

    async def perform_sync(self, sync_config: GetSyncConfigDetail) -> Dict[str, Any]:
        """æ‰§è¡ŒåŒæ­¥ä»»åŠ¡
        
        Args:
            sync_config: åŒæ­¥é…ç½®
            
        Returns:
            Dict[str, Any]: åŒæ­¥ç»“æœ
        """
        start_time = time.time()
        logger.info(f"å¼€å§‹æ‰§è¡ŒåŒæ­¥ä»»åŠ¡: {sync_config.id} - {sync_config.remark or 'æœªå‘½åä»»åŠ¡'}")
        
        account_schema: Optional[GetDriveAccountDetail] = None
        try:
            from backend.database.db import async_db_session
            from backend.app.coulddrive.crud.crud_drive_account import drive_account_dao
            
            logger.info(f"å°è¯•æ ¹æ®user_id={sync_config.user_id}ä»æ•°æ®åº“è·å–è´¦å·")
            async with async_db_session() as db:
                account_schema = await drive_account_dao.get(db, sync_config.user_id)
                
                if not account_schema:
                    error_msg = f"æœªæ‰¾åˆ°IDä¸º{sync_config.user_id}çš„è´¦å·ï¼Œæ— æ³•æ‰§è¡ŒåŒæ­¥ä»»åŠ¡"
                    logger.error(error_msg)
                    return {
                        "success": False,
                        "error": error_msg,
                        "elapsed_time": time.time() - start_time
                    }
                
                logger.info(f"æˆåŠŸè·å–åˆ°è´¦å·: {account_schema.username or account_schema.user_id} (ID: {account_schema.id})")
        except Exception as e:
            error_msg = f"è·å–è´¦å·æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return {
                "success": False,
                "error": error_msg,
                "elapsed_time": time.time() - start_time
            }
        
        # éªŒè¯è´¦å·å’Œé…ç½®å…³ç³»
        if sync_config.user_id != account_schema.id:
            error_msg = f"ä¸¥é‡çš„å†…éƒ¨é”™è¯¯: åŒæ­¥é…ç½® {sync_config.id} çš„è´¦å·ID({sync_config.user_id})ä¸è·å–åˆ°çš„è´¦å·ID({account_schema.id})ä¸åŒ¹é…"
            logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg,
                "elapsed_time": time.time() - start_time
            }
        
        # éªŒè¯è´¦å·å‚æ•°
        if not hasattr(account_schema, "cookies") or not account_schema.cookies:
            error_msg = f"è´¦å· {account_schema.id} ç¼ºå°‘cookieså­—æ®µï¼Œæ— æ³•æ‰§è¡ŒåŒæ­¥"
            logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg,
                "elapsed_time": time.time() - start_time
            }
        
        # éªŒè¯è´¦å·ç±»å‹
        if not hasattr(account_schema, "type") or not account_schema.type:
            error_msg = f"è´¦å· {account_schema.id} ç¼ºå°‘typeå­—æ®µï¼Œæ— æ³•ç¡®å®šç½‘ç›˜ç±»å‹"
            logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg,
                "elapsed_time": time.time() - start_time
            }
        
        # è·å–å…¨å±€ç½‘ç›˜ç®¡ç†å™¨
        drive_manager = get_drive_manager()
        
        try:
            # è§£ææºä¿¡æ¯
            src_meta = {}
            if sync_config.src_meta:
                try:
                    src_meta = json.loads(sync_config.src_meta)
                except json.JSONDecodeError:
                    logger.warning(f"è§£ææºå…ƒæ•°æ®å¤±è´¥: {sync_config.src_meta}")
            
            # è§£æç›®æ ‡ä¿¡æ¯
            dst_meta = {}
            if sync_config.dst_meta:
                try:
                    dst_meta = json.loads(sync_config.dst_meta)
                except json.JSONDecodeError:
                    logger.warning(f"è§£æç›®æ ‡å…ƒæ•°æ®å¤±è´¥: {sync_config.dst_meta}")
            
            # è§£ææ’é™¤å’Œé‡å‘½åè§„åˆ™
            exclude_rules = None
            if sync_config.exclude_rules:
                try:
                    exclude_rules = json.loads(sync_config.exclude_rules)
                except json.JSONDecodeError:
                    logger.warning(f"è§£ææ’é™¤è§„åˆ™å¤±è´¥: {sync_config.exclude_rules}")
            
            rename_rules = None
            if sync_config.rename_rules:
                try:
                    rename_rules = json.loads(sync_config.rename_rules)
                except json.JSONDecodeError:
                    logger.warning(f"è§£æé‡å‘½åè§„åˆ™å¤±è´¥: {sync_config.rename_rules}")
            
            # è§£æåŒæ­¥å‚æ•°
            sync_method = self._parse_sync_method(sync_config.method.value if hasattr(sync_config.method, 'value') else str(sync_config.method))
            recursion_speed = self._parse_recursion_speed(sync_config.speed)
            
            # æ„å»ºæºå®šä¹‰
            source_definition = ShareSourceDefinition(
                source_type=src_meta.get("source_type", "friend"),
                source_id=src_meta.get("source_id", ""),
                file_path=sync_config.src_path,
                ext_params=src_meta.get("ext_params", {})
            )
            
            # æ„å»ºç›®æ ‡å®šä¹‰
            target_definition = DiskTargetDefinition(
                file_path=sync_config.dst_path,
                file_id=dst_meta.get("file_id", "")
            )
            
            # è·å–ç½‘ç›˜ç±»å‹å­—ç¬¦ä¸²
            if isinstance(sync_config.type, DriveType):
                drive_type_str = sync_config.type.value
            else:
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                drive_type_str = sync_config.type
            
            # æ‰§è¡Œæ¯”è¾ƒé€»è¾‘
            comparison_result = await perform_comparison_logic(
                drive_manager=drive_manager,
                x_token=account_schema.cookies,
                source_definition=source_definition,
                target_definition=target_definition,
                recursive=True,  # å§‹ç»ˆé€’å½’å¤„ç†
                recursion_speed=recursion_speed,
                comparison_mode=sync_method,
                exclude_rules_def=exclude_rules,
                rename_rules_def=rename_rules,
                drive_type_str=drive_type_str
            )
            
            # åº”ç”¨æ¯”è¾ƒç»“æœ
            operation_results = await apply_comparison_operations(
                drive_manager=drive_manager,
                x_token=account_schema.cookies,
                comparison_result=comparison_result,
                drive_type_str=drive_type_str,
                sync_mode=sync_method
            )
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            stats = {
                "added_success": len(operation_results.get("add", {}).get("succeeded", [])),
                "added_fail": len(operation_results.get("add", {}).get("failed", [])),
                "deleted_success": len(operation_results.get("delete", {}).get("succeeded", [])),
                "deleted_fail": len(operation_results.get("delete", {}).get("failed", [])),
                "to_add_total": len(comparison_result.to_add),
                "to_delete_total": len(comparison_result.to_delete_from_target),
                "source_list_num": comparison_result.source_list_num,
                "target_list_num": comparison_result.target_list_num,
                "sync_method": sync_method,
                "recursion_speed": recursion_speed.value
            }
            
            elapsed_time = time.time() - start_time
            
            return {
                "success": True,
                "stats": stats,
                "details": operation_results,
                "elapsed_time": elapsed_time
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"åŒæ­¥ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}",
                "elapsed_time": time.time() - start_time
            }

    async def execute_sync_by_config_id(self, config_id: int, db: AsyncSession) -> Dict[str, Any]:
        """æ ¹æ®é…ç½®IDæ‰§è¡ŒåŒæ­¥ä»»åŠ¡"""
        from backend.app.coulddrive.crud.crud_filesync import sync_config_dao
        
        # ä½¿ç”¨ CRUD å±‚çš„éªŒè¯æ–¹æ³•
        sync_config, error_msg = await sync_config_dao.get_with_validation(db, config_id)
        if not sync_config:
            return {"success": False, "error": error_msg}
        
        if error_msg:  # é…ç½®å­˜åœ¨ä½†è¢«ç¦ç”¨
            return {"success": False, "error": error_msg}
        
        # å†…è”è½¬æ¢æ•°æ®åº“æ¨¡å‹åˆ°è¯¦æƒ… Schema
        def get_drive_type_from_db_value(db_value: str) -> DriveType:
            """ä»æ•°æ®åº“å€¼è·å– DriveType æšä¸¾"""
            try:
                return DriveType[db_value]
            except KeyError:
                for drive_type in DriveType:
                    if drive_type.value == db_value:
                        return drive_type
                raise ValueError(f"æ— æ•ˆçš„ç½‘ç›˜ç±»å‹: {db_value}ï¼Œæ”¯æŒçš„ç±»å‹: {[dt.value for dt in DriveType]}")
        
        # ä½¿ç”¨å­—å…¸æ–¹å¼åˆ›å»º schema å¯¹è±¡ï¼Œç¡®ä¿ field_validator ç”Ÿæ•ˆ
        try:
            # å°† SQLAlchemy å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸ï¼Œè§¦å‘ field_validator
            sync_config_dict = {
                'id': sync_config.id,
                'enable': sync_config.enable,
                'remark': sync_config.remark,
                'type': sync_config.type,  # è®© field_validator å¤„ç†ç±»å‹è½¬æ¢
                'src_path': sync_config.src_path,
                'src_meta': sync_config.src_meta,
                'dst_path': sync_config.dst_path,
                'dst_meta': sync_config.dst_meta,
                'user_id': sync_config.user_id,
                'cron': sync_config.cron,
                'speed': sync_config.speed,
                'method': sync_config.method,  # è®© field_validator å¤„ç†ç±»å‹è½¬æ¢
                'end_time': sync_config.end_time,
                'exclude': sync_config.exclude,
                'rename': sync_config.rename,
                'last_sync': sync_config.last_sync,
                'created_time': sync_config.created_time,
                'updated_time': sync_config.updated_time or sync_config.created_time,
                'created_by': getattr(sync_config, 'created_by', 1),
                'updated_by': getattr(sync_config, 'updated_by', 1)
            }
            sync_config_detail = GetSyncConfigDetail.model_validate(sync_config_dict)
        except Exception as e:
            # ä½¿ç”¨æ‰‹åŠ¨æ˜ å°„
            sync_config_detail = GetSyncConfigDetail(
                id=sync_config.id,
                enable=sync_config.enable,
                remark=sync_config.remark,
                type=get_drive_type_from_db_value(sync_config.type),
                src_path=sync_config.src_path,
                src_meta=sync_config.src_meta,
                dst_path=sync_config.dst_path,
                dst_meta=sync_config.dst_meta,
                user_id=sync_config.user_id,
                cron=sync_config.cron,
                speed=sync_config.speed,
                method=SyncMethod(sync_config.method) if hasattr(SyncMethod, sync_config.method.upper()) else SyncMethod.INCREMENTAL,
                end_time=sync_config.end_time,
                exclude=sync_config.exclude,
                rename=sync_config.rename,
                last_sync=sync_config.last_sync,
                created_time=sync_config.created_time,
                updated_time=sync_config.updated_time or sync_config.created_time,
                created_by=getattr(sync_config, 'created_by', 1),
                updated_by=getattr(sync_config, 'updated_by', 1)
            )
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¿‡æœŸ
        if sync_config_detail.end_time:
            from datetime import datetime
            current_time = datetime.now()
            if current_time > sync_config_detail.end_time:
                return {
                    "success": False,
                    "error": f"ä»»åŠ¡å·²è¿‡æœŸï¼Œå½“å‰æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S')}, ç»“æŸæ—¶é—´: {sync_config_detail.end_time.strftime('%Y-%m-%d %H:%M:%S')}"
                }
        
        # æ‰§è¡ŒåŒæ­¥ä»»åŠ¡
        return await self.perform_sync(sync_config_detail)

# åˆ›å»ºæœåŠ¡å•ä¾‹
file_sync_service = FileSyncService()

# ä»¥ä¸‹æ˜¯æ–‡ä»¶æ¯”è¾ƒå’ŒåŒæ­¥ç›¸å…³çš„ç±»å’Œå‡½æ•°

class ExclusionRule:
    def __init__(self,
                 pattern: str,
                 target: MatchTarget = MatchTarget.NAME,
                 item_type: ItemType = ItemType.ANY,
                 mode: MatchMode = MatchMode.CONTAINS,
                 case_sensitive: bool = False):
        self.pattern_str = pattern
        self.target = target
        self.item_type = item_type
        self.mode = mode
        self.case_sensitive = case_sensitive

        if not case_sensitive:
            self.pattern_str = self.pattern_str.lower()

        self._compiled_regex: Optional[Pattern] = None
        if self.mode == MatchMode.REGEX:
            try:
                self._compiled_regex = re.compile(self.pattern_str, 0 if self.case_sensitive else re.IGNORECASE)
            except re.error as e:
                raise ValueError(f"Invalid regex pattern '{self.pattern_str}': {e}")
        elif self.mode == MatchMode.WILDCARD:
            # Convert wildcard to regex
            # Basic conversion: escape regex chars, then replace * with .* and ? with .
            regex_pattern = re.escape(self.pattern_str).replace(r'\*', '.*').replace(r'\?', '.')
            self._compiled_regex = re.compile(regex_pattern, 0 if self.case_sensitive else re.IGNORECASE)

    def _get_value_to_match(self, item: BaseFileInfo) -> Optional[str]:
        value: Optional[str] = None
        if self.target == MatchTarget.NAME:
            value = item.file_name
        elif self.target == MatchTarget.PATH:
            value = item.file_path
        elif self.target == MatchTarget.EXTENSION:
            if not item.is_folder and '.' in item.file_name:
                value = item.file_name.rsplit('.', 1)[-1]
            else: # Folders or files without extension
                return None # Cannot match extension

        if value is not None and not self.case_sensitive:
            return value.lower()
        return value

    def matches(self, item: BaseFileInfo) -> bool:
        # 1. Check item type
        if self.item_type == ItemType.FILE and item.is_folder:
            return False
        if self.item_type == ItemType.FOLDER and not item.is_folder:
            return False

        # 2. Get value to match based on target
        value_to_match = self._get_value_to_match(item)
        if value_to_match is None and self.target == MatchTarget.EXTENSION: # e.g. folder when matching extension
             return False # Cannot match if target value is not applicable
        if value_to_match is None: # Should ideally not happen for NAME/PATH if item is valid
            return False

        # 3. Perform match based on mode
        if self.mode == MatchMode.EXACT:
            return value_to_match == self.pattern_str
        elif self.mode == MatchMode.CONTAINS:
            return self.pattern_str in value_to_match
        elif self.mode == MatchMode.REGEX or self.mode == MatchMode.WILDCARD:
            if self._compiled_regex:
                return bool(self._compiled_regex.search(value_to_match))
            return False # Should not happen if constructor worked for these modes

        return False

class ItemFilter:
    def __init__(self, exclusion_rules: Optional[List[ExclusionRule]] = None):
        self.exclusion_rules: List[ExclusionRule] = exclusion_rules or []

    def add_rule(self, rule: ExclusionRule):
        self.exclusion_rules.append(rule)

    def should_exclude(self, item: BaseFileInfo) -> bool:
        if not self.exclusion_rules:
            return False
        for rule in self.exclusion_rules:
            if rule.matches(item):
                return True
        return False

class RenameRule:
    def __init__(self,
                 match_regex: str,
                 replace_string: str,
                 target_scope: MatchTarget = MatchTarget.NAME, # NAME or PATH
                 case_sensitive: bool = False):
        self.match_regex_str = match_regex
        self.replace_string = replace_string # For re.sub()
        
        if target_scope not in [MatchTarget.NAME, MatchTarget.PATH]:
            raise ValueError("RenameRule target_scope must be MatchTarget.NAME or MatchTarget.PATH")
        self.target_scope = target_scope
        self.case_sensitive = case_sensitive
        
        try:
            self.compiled_regex = re.compile(
                self.match_regex_str, 0 if self.case_sensitive else re.IGNORECASE
            )
        except re.error as e:
            raise ValueError(f"Invalid regex pattern '{self.match_regex_str}' in RenameRule: {e}")

    def generate_new_path(self, item: BaseFileInfo) -> Optional[str]:
        """
        Applies the rename rule to an item and returns the new potential full path.
        Returns None if the rule doesn't change the relevant part or scope is not applicable.
        """
        original_path = item.file_path
        original_name = item.file_name

        if self.target_scope == MatchTarget.NAME:
            new_name = self.compiled_regex.sub(self.replace_string, original_name)
            if new_name == original_name:
                return None # No change in name

            # Reconstruct the full path if name changed
            if original_path == original_name: # Item is at root, path is just its name
                return new_name.replace("\\", "/") 
            
            try:
                # Use PurePosixPath for robust parent path detection and joining
                # Assumes item.file_path is in POSIX format or convertible
                path_obj = PurePosixPath(original_path)
                base_path = path_obj.parent
                # str(base_path) might be '.' if original_path has no directory part (e.g. "file.txt")
                # In such a case, new_path should just be new_name.
                if str(base_path) == "." and not '/' in original_path:
                    return new_name.replace("\\", "/")
                return str(base_path / new_name).replace("\\", "/")
            except Exception:
                 # Fallback for any path manipulation issue (should be rare with PurePosixPath)
                if original_path.endswith(original_name):
                    base_path_str = original_path[:-len(original_name)]
                    if not base_path_str and original_path != original_name: 
                        return new_name.replace("\\", "/")
                    # Ensure base_path_str ends with a slash if it's not empty and not already ending with slash
                    if base_path_str and not base_path_str.endswith('/'):
                         base_path_str += '/'
                    return (base_path_str + new_name).replace("\\", "/")
                return None # Cannot reliably determine base path

        elif self.target_scope == MatchTarget.PATH:
            new_path = self.compiled_regex.sub(self.replace_string, original_path)
            if new_path == original_path:
                return None # No change in path
            return new_path.replace("\\", "/")
        
        return None # Should not be reached if target_scope is validated

def compare_drive_lists(
    source_list: List[BaseFileInfo],
    target_list: List[BaseFileInfo],
    mode: str = "incremental",  
    rename_rules: Optional[List[RenameRule]] = None,
    source_base_path: str = "",  # æºç›®å½•åŸºç¡€è·¯å¾„å‚æ•°
    target_base_path: str = "",  # ç›®æ ‡ç›®å½•åŸºç¡€è·¯å¾„å‚æ•°
) -> Dict[str, Any]: 
    """
    æ¯”è¾ƒä¸¤ä¸ªæ–‡ä»¶åˆ—è¡¨ï¼Œè¯†åˆ«æ·»åŠ ã€æ›´æ–°ã€åˆ é™¤å’Œé‡å‘½åæ“ä½œ
    
    :param source_list: æºç›®å½•æ–‡ä»¶åˆ—è¡¨
    :param target_list: ç›®æ ‡ç›®å½•æ–‡ä»¶åˆ—è¡¨
    :param mode: æ¯”è¾ƒæ¨¡å¼ï¼Œ"incremental" æˆ– "full_sync"
    :param rename_rules: é‡å‘½åè§„åˆ™åˆ—è¡¨
    :param source_base_path: æºç›®å½•çš„åŸºç¡€è·¯å¾„ï¼Œç”¨äºè®¡ç®—ç›¸å¯¹è·¯å¾„
    :param target_base_path: ç›®æ ‡ç›®å½•çš„åŸºç¡€è·¯å¾„ï¼Œç”¨äºè®¡ç®—ç›¸å¯¹è·¯å¾„
    :return: æ¯”è¾ƒç»“æœå­—å…¸ï¼ŒåŒ…å« to_add, to_update_in_target, to_delete_from_target, to_rename_in_target å­—æ®µ
    """
    def get_relative_path(full_path: str, base_path: str) -> str:
        """è·å–ç›¸å¯¹è·¯å¾„"""
        if not base_path:
            return full_path
        # ç¡®ä¿è·¯å¾„ä»¥/å¼€å¤´
        full_path = full_path if full_path.startswith('/') else '/' + full_path
        base_path = base_path if base_path.startswith('/') else '/' + base_path
        # ç§»é™¤ç»“å°¾çš„/
        base_path = base_path.rstrip('/')
        if full_path.startswith(base_path):
            return full_path[len(base_path):]
        return full_path

    def calculate_target_path_and_parent(source_item: BaseFileInfo) -> Tuple[str, str]:
        """
        è®¡ç®—ç›®æ ‡å®Œæ•´è·¯å¾„å’Œç›®æ ‡çˆ¶ç›®å½•è·¯å¾„
        
        :param source_item: æºæ–‡ä»¶ä¿¡æ¯
        :return: (ç›®æ ‡å®Œæ•´è·¯å¾„, ç›®æ ‡çˆ¶ç›®å½•è·¯å¾„)
        """
        # è·å–æºæ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
        relative_path = get_relative_path(source_item.file_path, source_base_path)
        
        # è®¡ç®—ç›®æ ‡å®Œæ•´è·¯å¾„
        target_full_path = f"{target_base_path}/{relative_path}".replace("//", "/")
        
        # è®¡ç®—ç›®æ ‡çˆ¶ç›®å½•è·¯å¾„
        target_parent_path = "/".join(target_full_path.split("/")[:-1]) or "/"
        
        return target_full_path, target_parent_path

    # è§„èŒƒåŒ–åŸºç¡€è·¯å¾„
    source_base_path = source_base_path.rstrip('/')
    target_base_path = target_base_path.rstrip('/')

    # æ„å»ºç›®æ ‡è·¯å¾„åˆ°file_idçš„æ˜ å°„ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
    target_path_to_file_id = {item.file_path: item.file_id for item in target_list if item.file_path and item.file_id}
    
    # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
    to_add = []
    to_delete_from_target = []
    to_update = []
    to_rename = []
    
    # å¤„ç†æºæ–‡ä»¶åˆ—è¡¨ï¼Œæ‰¾å‡ºéœ€è¦æ·»åŠ æˆ–æ›´æ–°çš„æ–‡ä»¶
    for src_item in source_list:
        # åº”ç”¨é‡å‘½åè§„åˆ™
        if rename_rules:
            for rule in rename_rules:
                new_path = rule.generate_new_path(src_item)
                if new_path:
                    # åˆ›å»ºæ–°çš„æ–‡ä»¶ä¿¡æ¯å¯¹è±¡ï¼Œä½¿ç”¨é‡å‘½ååçš„è·¯å¾„
                    src_item = BaseFileInfo(
                        file_id=src_item.file_id,
                        file_name=os.path.basename(new_path),
                        file_path=new_path,
                        file_size=src_item.file_size,
                        is_folder=src_item.is_folder,
                        created_time=src_item.created_time,
                        updated_time=src_item.updated_time
                    )
                    break
        
        # è®¡ç®—ç›®æ ‡è·¯å¾„å’Œçˆ¶ç›®å½•è·¯å¾„
        target_full_path, target_parent_path = calculate_target_path_and_parent(src_item)
        
        # æŸ¥æ‰¾ç›®æ ‡çˆ¶ç›®å½•çš„file_id
        target_parent_file_id = target_path_to_file_id.get(target_parent_path)
        
        if not target_parent_file_id:
            # å‘ä¸ŠæŸ¥æ‰¾å·²å­˜åœ¨çš„ç›®å½•
            search_steps = []
            current_path = target_parent_path
            while current_path and current_path != "/" and current_path != target_base_path:
                search_steps.append(current_path)
                if current_path in target_path_to_file_id:
                    target_parent_file_id = target_path_to_file_id[current_path]
                    break
                # å‘ä¸Šä¸€çº§ç›®å½•
                current_path = "/".join(current_path.split("/")[:-1]) or "/"
            
            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨æ ¹ç›®å½•
            if not target_parent_file_id and target_base_path in target_path_to_file_id:
                target_parent_file_id = target_path_to_file_id[target_base_path]
        
        if not target_parent_file_id:
            logger.warning(f"âŒ æ— æ³•æ‰¾åˆ°ç›®æ ‡çˆ¶ç›®å½•çš„file_id: {target_parent_path}ï¼Œå°†åœ¨ä¼ è¾“æ—¶æŠ¥é”™")
            continue

        # æ„å»ºå¢å¼ºçš„æ·»åŠ é¡¹ä¿¡æ¯
        add_item = {
            "source_item": src_item,
            "target_full_path": target_full_path,
            "target_parent_path": target_parent_path,
            "target_parent_file_id": target_parent_file_id
        }
        to_add.append(add_item)

    # æ ¹æ®åŒæ­¥æ¨¡å¼å¤„ç†åˆ é™¤æ“ä½œ
    if mode == SyncMethod.FULL.value:
        # å®Œå…¨åŒæ­¥ï¼šåˆ é™¤ç›®æ ‡ä¸­å¤šä½™çš„æ–‡ä»¶ï¼ˆæºä¸­ä¸å­˜åœ¨çš„æ–‡ä»¶ï¼‰
        for target_rel_path, target_item in target_path_to_file_id.items():
            if target_rel_path not in target_path_to_file_id:
                to_delete_from_target.append(target_item)
    elif mode == SyncMethod.OVERWRITE.value:
        # è¦†ç›–åŒæ­¥ï¼šåˆ é™¤ç›®æ ‡ç›®å½•é‡Œçš„æ‰€æœ‰æ–‡ä»¶ï¼Œç„¶åä¿å­˜æºç›®å½•é‡Œçš„æ‰€æœ‰æ–‡ä»¶
        # 1. å°†æ‰€æœ‰ç›®æ ‡æ–‡ä»¶æ ‡è®°ä¸ºåˆ é™¤
        for target_rel_path, target_item in target_path_to_file_id.items():
            to_delete_from_target.append(target_item)
        
        # 2. å°†æ‰€æœ‰æºæ–‡ä»¶æ ‡è®°ä¸ºæ·»åŠ ï¼ˆæ¸…ç©ºä¹‹å‰çš„æ·»åŠ åˆ—è¡¨ï¼Œé‡æ–°æ·»åŠ æ‰€æœ‰æºæ–‡ä»¶ï¼‰
        to_add = []  # æ¸…ç©ºä¹‹å‰çš„æ·»åŠ åˆ—è¡¨
        to_update = []  # æ¸…ç©ºæ›´æ–°åˆ—è¡¨ï¼Œè¦†ç›–æ¨¡å¼ä¸éœ€è¦æ›´æ–°
        to_rename = []  # æ¸…ç©ºé‡å‘½ååˆ—è¡¨ï¼Œè¦†ç›–æ¨¡å¼ä¸éœ€è¦é‡å‘½å
        
        for src_item in source_list:
            # è®¡ç®—ç›®æ ‡è·¯å¾„ä¿¡æ¯
            target_full_path, target_parent_path = calculate_target_path_and_parent(src_item)
            
            # åœ¨è¦†ç›–æ¨¡å¼ä¸‹ï¼Œç›®æ ‡çˆ¶ç›®å½•çš„file_idéœ€è¦é‡æ–°è®¡ç®—ï¼ˆå› ä¸ºç›®æ ‡ç›®å½•ä¼šè¢«æ¸…ç©ºï¼‰
            target_parent_file_id = target_path_to_file_id.get(target_base_path)  # ä½¿ç”¨æ ¹ç›®å½•çš„file_id
            
            # æ„å»ºæ·»åŠ é¡¹ä¿¡æ¯
            add_item = {
                "source_item": src_item,
                "target_full_path": target_full_path,
                "target_parent_path": target_parent_path,
                "target_parent_file_id": target_parent_file_id
            }
            to_add.append(add_item)
    
    return {
        "to_add": to_add,
        "to_update_in_target": to_update,
        "to_delete_from_target": to_delete_from_target,
        "to_rename_in_target": to_rename
    }

def _parse_exclusion_rules(rules_def: Optional[List[ExclusionRuleDefinition]]) -> Optional[ItemFilter]:
    if not rules_def:
        return None
    item_filter = ItemFilter()
    for i, rule_data in enumerate(rules_def):
        try:
            item_filter.add_rule(ExclusionRule(
                pattern=rule_data.pattern,
                target=rule_data.target,
                item_type=rule_data.item_type,
                mode=rule_data.mode,
                case_sensitive=rule_data.case_sensitive
            ))
        except ValueError as e:
            # æŠ›å‡ºä¸€ä¸ªç‰¹å®šçš„é”™è¯¯ï¼Œå¯ä»¥è¢« API å±‚æ•è·
            raise ValueError(f"æ’é™¤è§„åˆ™ #{i+1} ('{rule_data.pattern}') æ ¼å¼é”™è¯¯: {e}")
    return item_filter

def _parse_rename_rules(rules_def: Optional[List[RenameRuleDefinition]]) -> Optional[List[RenameRule]]:
    if not rules_def:
        return None
    parsed_rules = []
    for i, rule_data in enumerate(rules_def):
        try:
            parsed_rules.append(RenameRule(
                match_regex=rule_data.match_regex,
                replace_string=rule_data.replace_string,
                target_scope=rule_data.target_scope,
                case_sensitive=rule_data.case_sensitive
            ))
        except ValueError as e:
            # Raise a specific error that can be caught by the API layer
            raise ValueError(f"é‡å‘½åè§„åˆ™ #{i+1} ('{rule_data.match_regex}') æ ¼å¼é”™è¯¯: {e}")
    return parsed_rules

async def _create_missing_target_directories(
    drive_manager: Any,
    x_token: str,
    to_add: List[Dict[str, Any]],
    target_definition: DiskTargetDefinition,
    drive_type_str: str,
    target_path_to_file_id: Optional[Dict[str, str]] = None
) -> None:
    """
    åœ¨æ¯”è¾ƒé˜¶æ®µåˆ›å»ºç¼ºå¤±çš„ç›®æ ‡ç›®å½•
    
    :param drive_manager: ç½‘ç›˜ç®¡ç†å™¨å®ä¾‹
    :param x_token: è®¤è¯ä»¤ç‰Œ
    :param to_add: å¾…æ·»åŠ é¡¹ç›®åˆ—è¡¨
    :param target_definition: ç›®æ ‡å®šä¹‰
    :param drive_type_str: ç½‘ç›˜ç±»å‹å­—ç¬¦ä¸²
    :param target_path_to_file_id: ç›®æ ‡è·¯å¾„åˆ°file_idçš„æ˜ å°„ï¼Œç”¨äºæŸ¥æ‰¾å·²å­˜åœ¨çš„ç›®å½•
    """
    # æ”¶é›†æ‰€æœ‰éœ€è¦åˆ›å»ºçš„ç›®å½•è·¯å¾„ï¼ˆæ£€æŸ¥ç›®æ ‡çˆ¶ç›®å½•æ˜¯å¦çœŸå®å­˜åœ¨ï¼‰
    missing_dirs = set()
    for add_item in to_add:
        target_parent_path = add_item.get("target_parent_path")
        target_parent_file_id = add_item.get("target_parent_file_id")
        
        if target_parent_path and target_parent_path != "/" and target_parent_path != target_definition.file_path:
            # æ£€æŸ¥ç›®æ ‡çˆ¶ç›®å½•æ˜¯å¦çœŸå®å­˜åœ¨äºç›®æ ‡è·¯å¾„æ˜ å°„ä¸­
            if not target_path_to_file_id or target_parent_path not in target_path_to_file_id:
                missing_dirs.add(target_parent_path)
    
    if not missing_dirs:
        return
    
    logger.info(f"ğŸ” [ç›®å½•åˆ›å»º] å‘ç° {len(missing_dirs)} ä¸ªç¼ºå¤±ç›®å½•éœ€è¦åˆ›å»º")
    
    # æŒ‰è·¯å¾„æ·±åº¦æ’åºï¼Œç¡®ä¿å…ˆåˆ›å»ºçˆ¶ç›®å½•
    sorted_dirs = sorted(missing_dirs, key=lambda x: x.count('/'))
    
    # è®°å½•å·²åˆ›å»ºçš„ç›®å½•æ˜ å°„
    created_dir_to_file_id = {}
    
    for dir_path in sorted_dirs:
        try:
            # ç¡®ä¿è·¯å¾„æ ¼å¼ä¸€è‡´
            normalized_dir_path = dir_path.replace("\\", "/")
            
            # è·å–çˆ¶ç›®å½•è·¯å¾„å’Œç›®å½•åç§°
            parent_dir_path = "/".join(normalized_dir_path.split("/")[:-1]) or "/"
            dir_name = normalized_dir_path.split("/")[-1]
            
            # æŸ¥æ‰¾çˆ¶ç›®å½•çš„file_id
            parent_file_id = None
            if target_path_to_file_id and parent_dir_path in target_path_to_file_id:
                parent_file_id = target_path_to_file_id[parent_dir_path]
            elif parent_dir_path in created_dir_to_file_id:
                parent_file_id = created_dir_to_file_id[parent_dir_path]
            
            if not parent_file_id:
                logger.warning(f"âŒ [ç›®å½•åˆ›å»º] æ— æ³•æ‰¾åˆ°çˆ¶ç›®å½•file_id: {parent_dir_path}")
                continue
            
            # æ„å»º MkdirParam
            mkdir_params = MkdirParam(
                drive_type=drive_type_str,
                file_path=normalized_dir_path,
                parent_id=parent_file_id,
                file_name=dir_name
            )
            
            # åˆ›å»ºç›®å½•
            new_dir_info = await drive_manager.create_mkdir(x_token, mkdir_params)
            if new_dir_info and hasattr(new_dir_info, 'file_id'):
                created_dir_to_file_id[normalized_dir_path] = new_dir_info.file_id
                logger.info(f"âœ… [ç›®å½•åˆ›å»º] åˆ›å»ºç›®å½•æˆåŠŸ: {normalized_dir_path} (file_id: {new_dir_info.file_id})")
            else:
                logger.warning(f"âŒ [ç›®å½•åˆ›å»º] åˆ›å»ºç›®å½•å¤±è´¥: {normalized_dir_path}")
                
        except Exception as e:
            logger.error(f"âŒ [ç›®å½•åˆ›å»º] åˆ›å»ºç›®å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    # æ›´æ–°to_addä¸­çš„target_parent_file_id
    updated_count = 0
    for add_item in to_add:
        try:
            target_parent_path = add_item.get("target_parent_path")
            if target_parent_path:
                # ç¡®ä¿è·¯å¾„æ ¼å¼ä¸€è‡´
                normalized_target_parent_path = target_parent_path.replace("\\", "/")
                if normalized_target_parent_path in created_dir_to_file_id:
                    # æ›´æ–°ä¸ºæ–°åˆ›å»ºçš„ç›®å½•ID
                    old_file_id = add_item.get("target_parent_file_id")
                    new_file_id = created_dir_to_file_id[normalized_target_parent_path]
                    add_item["target_parent_file_id"] = new_file_id
                    updated_count += 1
                elif target_path_to_file_id and normalized_target_parent_path in target_path_to_file_id:
                    # ä½¿ç”¨å·²å­˜åœ¨ç›®å½•çš„ID
                    existing_file_id = target_path_to_file_id[normalized_target_parent_path]
                    if add_item.get("target_parent_file_id") != existing_file_id:
                        old_file_id = add_item.get("target_parent_file_id")
                        add_item["target_parent_file_id"] = existing_file_id
                        updated_count += 1
        except Exception as e:
            logger.error(f"âŒ [ç›®å½•åˆ›å»º] æ›´æ–°æ–‡ä»¶çˆ¶ç›®å½•IDæ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    if updated_count > 0:
        logger.info(f"âœ… [ç›®å½•åˆ›å»º] æˆåŠŸæ›´æ–°äº† {updated_count} ä¸ªæ–‡ä»¶çš„çˆ¶ç›®å½•file_id")

async def _get_list_for_compare_op(
    drive_manager: Any,
    x_token: str,
    is_source: bool,
    definition: Union[ShareSourceDefinition, DiskTargetDefinition],
    top_level_recursive: bool,
    top_level_recursion_speed: RecursionSpeed,
    item_filter_instance: Optional[ItemFilter],
    drive_type_str: str
) -> Tuple[List[BaseFileInfo], float]:
    """
    è·å–åˆ—è¡¨æ•°æ®ç”¨äºæ¯”è¾ƒæ“ä½œ
    
    :param drive_manager: ç½‘ç›˜ç®¡ç†å™¨å®ä¾‹
    :param x_token: è®¤è¯ä»¤ç‰Œ
    :param is_source: æ˜¯å¦ä¸ºæºç«¯æ•°æ®
    :param definition: è·¯å¾„å®šä¹‰ï¼ˆæºæˆ–ç›®æ ‡ï¼‰
    :param top_level_recursive: æ˜¯å¦é€’å½’
    :param top_level_recursion_speed: é€’å½’é€Ÿåº¦
    :param item_filter_instance: é¡¹ç›®è¿‡æ»¤å™¨å®ä¾‹
    :param drive_type_str: ç½‘ç›˜ç±»å‹å­—ç¬¦ä¸²
    :return: æ–‡ä»¶åˆ—è¡¨å’Œè€—æ—¶
    """
    start_time = time.time()
    result_list: List[BaseFileInfo] = []
    
    # ç¡®ä¿å¼‚æ­¥æ–¹æ³•ä½¿ç”¨ await è°ƒç”¨
    if is_source:
        source_def = definition
        
        # æ„å»º ListShareFilesParam
        params = ListShareFilesParam(
            drive_type=drive_type_str,
            source_type=source_def.source_type,
            source_id=source_def.source_id,
            file_path=source_def.file_path,
            recursive=top_level_recursive,
            recursion_speed=top_level_recursion_speed
        )
        
        # ä½¿ç”¨ç»Ÿä¸€çš„è°ƒç”¨æ–¹å¼
        result_list = await drive_manager.get_share_list(x_token, params)
    else:
        target_def = definition
        
        # æ„å»º ListFilesParam
        params = ListFilesParam(
            drive_type=drive_type_str,
            file_path=target_def.file_path,
            file_id=target_def.file_id,
            recursive=top_level_recursive,
            recursion_speed=top_level_recursion_speed
        )
        
        # ä½¿ç”¨ç»Ÿä¸€çš„è°ƒç”¨æ–¹å¼
        result_list = await drive_manager.get_disk_list(x_token, params)
    
    # åº”ç”¨è¿‡æ»¤å™¨
    if item_filter_instance:
        result_list = [item for item in result_list if not item_filter_instance.should_exclude(item)]
    
    elapsed_time = time.time() - start_time
    return result_list, elapsed_time

async def perform_comparison_logic(
    drive_manager: Any,
    x_token: str,
    source_definition: ShareSourceDefinition,
    target_definition: DiskTargetDefinition,
    recursive: bool,
    recursion_speed: RecursionSpeed,
    comparison_mode: str, 
    exclude_rules_def: Optional[List[ExclusionRuleDefinition]],
    rename_rules_def: Optional[List[RenameRuleDefinition]],
    drive_type_str: str
) -> GetCompareDetail:
    """
    æ‰§è¡Œæ¯”è¾ƒé€»è¾‘
    
    :param drive_manager: ç½‘ç›˜ç®¡ç†å™¨å®ä¾‹
    :param x_token: è®¤è¯ä»¤ç‰Œ
    :param source_definition: æºå®šä¹‰
    :param target_definition: ç›®æ ‡å®šä¹‰
    :param recursive: æ˜¯å¦é€’å½’
    :param recursion_speed: é€’å½’é€Ÿåº¦
    :param comparison_mode: æ¯”è¾ƒæ¨¡å¼
    :param exclude_rules_def: æ’é™¤è§„åˆ™å®šä¹‰
    :param rename_rules_def: é‡å‘½åè§„åˆ™å®šä¹‰
    :param drive_type_str: ç½‘ç›˜ç±»å‹å­—ç¬¦ä¸²
    :return: æ¯”è¾ƒç»“æœè¯¦æƒ…
    """
    
    common_item_filter = _parse_exclusion_rules(exclude_rules_def)
    
    # è¦†ç›–æ¨¡å¼çš„ç®€åŒ–å¤„ç†ï¼šåªå¤„ç†ä¸€å±‚ç›®å½•ï¼Œä¸é€’å½’
    if comparison_mode == SyncMethod.OVERWRITE.value:
        # 1. åªè·å–æºç›®å½•ä¸‹çš„ä¸€å±‚æ–‡ä»¶åˆ—è¡¨ï¼ˆä¸é€’å½’ï¼‰
        source_list, source_time = await _get_list_for_compare_op(
            drive_manager=drive_manager,
            x_token=x_token,
            is_source=True,
            definition=source_definition,
            top_level_recursive=False,  # è¦†ç›–æ¨¡å¼ä¸é€’å½’
            top_level_recursion_speed=recursion_speed,
            item_filter_instance=common_item_filter,
            drive_type_str=drive_type_str
        )
        
        # 2. åªè·å–ç›®æ ‡ç›®å½•ä¸‹çš„ä¸€å±‚æ–‡ä»¶åˆ—è¡¨ï¼ˆä¸é€’å½’ï¼Œç”¨äºåˆ é™¤ï¼‰
        target_list, target_time = await _get_list_for_compare_op(
            drive_manager=drive_manager,
            x_token=x_token,
            is_source=False,
            definition=target_definition,
            top_level_recursive=False,  # è¦†ç›–æ¨¡å¼ä¸é€’å½’
            top_level_recursion_speed=recursion_speed,
            item_filter_instance=None,  # åˆ é™¤æ—¶ä¸åº”ç”¨è¿‡æ»¤å™¨ï¼Œåˆ é™¤æ‰€æœ‰æ–‡ä»¶
            drive_type_str=drive_type_str
        )
        
        # 3. æ„å»ºç®€åŒ–çš„æ¯”è¾ƒç»“æœï¼šåˆ é™¤æ‰€æœ‰ç›®æ ‡æ–‡ä»¶ï¼Œæ·»åŠ æ‰€æœ‰æºæ–‡ä»¶
        comparison_result = {
            "to_add": [],
            "to_update_in_target": [],
            "to_delete_from_target": target_list,  # åˆ é™¤æ‰€æœ‰ç›®æ ‡æ–‡ä»¶
            "to_rename_in_target": []
        }
        
        # 4. å°†æ‰€æœ‰æºæ–‡ä»¶æ ‡è®°ä¸ºæ·»åŠ ï¼ˆåªå¤„ç†ä¸€å±‚ï¼Œç›´æ¥è½¬å­˜åˆ°ç›®æ ‡ç›®å½•ï¼‰
        for src_item in source_list:
            # è¦†ç›–æ¨¡å¼ï¼šç›´æ¥å°†æºæ–‡ä»¶è½¬å­˜åˆ°ç›®æ ‡ç›®å½•ï¼Œä½¿ç”¨åŸæ–‡ä»¶å
            target_full_path = target_definition.file_path + "/" + src_item.file_name
            
            target_parent_file_id = target_definition.file_id
            
            add_item = {
                "source_item": src_item,
                "target_full_path": target_full_path,
                "target_parent_path": target_definition.file_path,
                "target_parent_file_id": target_parent_file_id
            }
            comparison_result["to_add"].append(add_item)
        
        # 5. æ„å»ºè¿”å›æ•°æ®
        compare_detail_data = {
            "drive_type": drive_type_str,
            "source_list_num": len(source_list),
            "target_list_num": len(target_list),
            "source_list_time": source_time,
            "target_list_time": target_time,
            "source_definition": source_definition,
            "target_definition": target_definition,
            **comparison_result
        }
        
        return GetCompareDetail(**compare_detail_data)
    
    # å¢é‡åŒæ­¥å’Œå®Œå…¨åŒæ­¥çš„æ­£å¸¸æ¯”è¾ƒé€»è¾‘
    source_list, source_time = await _get_list_for_compare_op(
        drive_manager=drive_manager,
        x_token=x_token,
        is_source=True,
        definition=source_definition,
        top_level_recursive=recursive,
        top_level_recursion_speed=recursion_speed,
        item_filter_instance=common_item_filter,
        drive_type_str=drive_type_str
    )
    
    target_list, target_time = await _get_list_for_compare_op(
        drive_manager=drive_manager,
        x_token=x_token,
        is_source=False,
        definition=target_definition,
        top_level_recursive=recursive,
        top_level_recursion_speed=recursion_speed,
        item_filter_instance=common_item_filter,
        drive_type_str=drive_type_str
    )
    
    parsed_rename_rules = _parse_rename_rules(rename_rules_def)

    # æ„å»ºç›®æ ‡è·¯å¾„åˆ°file_idçš„æ˜ å°„
    target_path_to_file_id = {}
    for target_item in target_list:
        if target_item.file_path and target_item.file_id:
            target_path_to_file_id[target_item.file_path] = target_item.file_id

    # compare_drive_lists è¿”å›åŸºç¡€çš„æ¯”è¾ƒç»“æœå­—å…¸
    comparison_result = compare_drive_lists(
        source_list=source_list,
        target_list=target_list,
        mode=comparison_mode,
        rename_rules=parsed_rename_rules,
        source_base_path=source_definition.file_path,
        target_base_path=target_definition.file_path
    )
    
    # åœ¨æ¯”è¾ƒé˜¶æ®µåˆ›å»ºç¼ºå¤±çš„ç›®æ ‡ç›®å½•
    await _create_missing_target_directories(
        drive_manager=drive_manager,
        x_token=x_token,
        to_add=comparison_result.get('to_add', []),
        target_definition=target_definition,
        drive_type_str=drive_type_str,
        target_path_to_file_id=target_path_to_file_id
    )
    
    # æ„å»ºå®Œæ•´çš„ GetCompareDetail å¯¹è±¡æ‰€éœ€çš„æ•°æ®
    compare_detail_data = {
        "drive_type": drive_type_str,
        "source_list_num": len(source_list),
        "target_list_num": len(target_list),
        "source_list_time": source_time,
        "target_list_time": target_time,
        "source_definition": source_definition,
        "target_definition": target_definition,
        # æ·»åŠ æ¯”è¾ƒç»“æœçš„æ ¸å¿ƒå­—æ®µ
        **comparison_result
    }
    
    # è¿”å› GetCompareDetail æ¨¡å‹å®ä¾‹
    return GetCompareDetail(**compare_detail_data)

async def apply_comparison_operations(
    drive_manager: Any,
    x_token: str,
    comparison_result: GetCompareDetail,
    drive_type_str: str,
    sync_mode: str = "incremental"
) -> Dict[str, Dict[str, List[str]]]:
    """
    æ ¹æ®æ¯”è¾ƒç»“æœæ‰§è¡Œç›¸åº”çš„æ“ä½œï¼ˆæ·»åŠ ã€åˆ é™¤ã€é‡å‘½åã€æ›´æ–°ï¼‰

    :param drive_manager: ç½‘ç›˜ç®¡ç†å™¨å®ä¾‹
    :param x_token: è®¤è¯ä»¤ç‰Œ
    :param comparison_result: æ¯”è¾ƒç»“æœï¼ŒåŒ…å«to_addã€to_delete_from_targetç­‰æ“ä½œåˆ—è¡¨
    :param drive_type_str: ç½‘ç›˜ç±»å‹å­—ç¬¦ä¸²
    :return: å„ç±»æ“ä½œçš„ç»“æœï¼Œæ ¼å¼ä¸º:
        {
            "add": {"succeeded": [...], "failed": [...]},
            "delete": {"succeeded": [...], "failed": [...]}
        }
    """
    operation_results = {
        "add": {"succeeded": [], "failed": []},
        "delete": {"succeeded": [], "failed": []}
    }

    # å¤„ç†æ·»åŠ æ“ä½œ
    if comparison_result.to_add:
        add_results = await _process_add_operations(
            drive_manager=drive_manager,
            x_token=x_token,
            to_add=comparison_result.to_add,
            source_definition=comparison_result.source_definition,
            target_definition=comparison_result.target_definition,
            drive_type_str=drive_type_str,
            sync_mode=sync_mode
        )
        operation_results["add"] = add_results

    # å¤„ç†åˆ é™¤æ“ä½œ
    if comparison_result.to_delete_from_target:
        delete_results = await _process_delete_operations(
            drive_manager=drive_manager,
            x_token=x_token,
            to_delete=comparison_result.to_delete_from_target,
            drive_type_str=drive_type_str
        )
        operation_results["delete"] = delete_results

    return operation_results

async def _process_add_operations(
    drive_manager: Any,
    x_token: str,
    to_add: List[Dict[str, Any]],  # ä¿®æ”¹ç±»å‹ï¼Œç°åœ¨æ˜¯åŒ…å«å®Œæ•´ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    source_definition: ShareSourceDefinition,
    target_definition: DiskTargetDefinition,
    drive_type_str: str,
    sync_mode: str = "incremental",
    ext_transfer_params: Optional[Dict[str, Any]] = None
) -> Dict[str, List[str]]:
    """
    å¤„ç†æ·»åŠ æ“ä½œï¼ŒåŒ…æ‹¬åˆ›å»ºç›®å½•å’Œä¼ è¾“æ–‡ä»¶
    
    :param drive_manager: ç½‘ç›˜ç®¡ç†å™¨å®ä¾‹
    :param x_token: è®¤è¯ä»¤ç‰Œ
    :param to_add: è¦æ·»åŠ çš„æ–‡ä»¶/ç›®å½•åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«source_itemã€target_full_pathã€target_parent_pathç­‰ä¿¡æ¯
    :param source_definition: æºå®šä¹‰
    :param target_definition: ç›®æ ‡å®šä¹‰
    :param drive_type_str: ç½‘ç›˜ç±»å‹å­—ç¬¦ä¸²
    :param ext_transfer_params: é¢å¤–çš„ä¼ è¾“å‚æ•°
    :return: æ“ä½œç»“æœï¼ŒåŒ…å«succeededå’Œfailedä¸¤ä¸ªåˆ—è¡¨
    """
    operation_results = {'succeeded': [], 'failed': []}

    # ğŸ”§ åœ¨å¤„ç†æ–‡ä»¶è½¬å­˜ä¹‹å‰ï¼Œå…ˆåˆ›å»ºç¼ºå¤±çš„ç›®å½•
    # ä» to_add åˆ—è¡¨ä¸­æ„å»ºå·²çŸ¥çš„ç›®æ ‡è·¯å¾„æ˜ å°„
    existing_target_path_to_file_id = {}
    for add_item in to_add:
        if add_item.get("target_parent_file_id") and add_item.get("target_parent_path"):
            existing_target_path_to_file_id[add_item["target_parent_path"]] = add_item["target_parent_file_id"]
    
    try:
        await _create_missing_target_directories(
            drive_manager=drive_manager,
            x_token=x_token,
            to_add=to_add,
            target_definition=target_definition,
            drive_type_str=drive_type_str,
            target_path_to_file_id=existing_target_path_to_file_id
        )
        logger.info(f"âœ… ç¼ºå¤±ç›®å½•åˆ›å»ºå®Œæˆï¼Œå¼€å§‹å¤„ç†æ–‡ä»¶è½¬å­˜")
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºç¼ºå¤±ç›®å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        # å³ä½¿åˆ›å»ºç›®å½•å¤±è´¥ï¼Œä¹Ÿç»§ç»­å¤„ç†æ–‡ä»¶è½¬å­˜

    # æå–source_itemè¿›è¡Œæ’åº
    sorted_to_add = sorted(to_add, key=lambda add_item: add_item["source_item"].file_path)

    # æŒ‰ç›®æ ‡çˆ¶ç›®å½•åˆ†ç»„æ–‡ä»¶
    files_to_transfer_by_target_parent: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    for add_item in sorted_to_add:
        source_item = add_item["source_item"]
        # è¦†ç›–æ¨¡å¼å¤„ç†æ‰€æœ‰ç±»å‹çš„æ–‡ä»¶ï¼Œå…¶ä»–æ¨¡å¼åªå¤„ç†éæ–‡ä»¶å¤¹
        if sync_mode == SyncMethod.OVERWRITE.value or not source_item.is_folder:
            target_parent_path = add_item["target_parent_path"]
            files_to_transfer_by_target_parent[target_parent_path].append(add_item)
    
    for target_parent_dir, add_items_in_group in files_to_transfer_by_target_parent.items():
        if not add_items_in_group:
            continue

        # ç¡®ä¿ç›®æ ‡è·¯å¾„ä½¿ç”¨æ­£æ–œæ 
        normalized_target_parent_dir = os.path.normpath(target_parent_dir).replace("\\", "/")
        
        try:
            source_fs_ids_to_transfer = [add_item["source_item"].file_id for add_item in add_items_in_group]
            
            current_transfer_ext_params = {}
            if ext_transfer_params:
                current_transfer_ext_params = {**ext_transfer_params}
                        
            # ä¼ é€’æ‰€æœ‰æ–‡ä»¶çš„å®Œæ•´ä¿¡æ¯ï¼Œè®©å…·ä½“çš„ç½‘ç›˜å®¢æˆ·ç«¯å¤„ç†
            # å°†æ‰€æœ‰æ–‡ä»¶çš„ file_ext ä¿¡æ¯ä¼ é€’ç»™å®¢æˆ·ç«¯
            files_ext_info = []
            for add_item in add_items_in_group:
                source_item = add_item["source_item"]
                file_ext_info = {
                    'file_id': source_item.file_id,
                    'file_ext': source_item.file_ext if hasattr(source_item, 'file_ext') else {},
                    'parent_id': source_item.parent_id
                }
                files_ext_info.append(file_ext_info)
            
            # å°†æ–‡ä»¶æ‰©å±•ä¿¡æ¯ä¼ é€’ç»™å®¢æˆ·ç«¯
            current_transfer_ext_params['files_ext_info'] = files_ext_info
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„å…¬å…±å‚æ•°ä½œä¸ºåŸºç¡€å‚æ•°
            first_source_item = add_items_in_group[0]["source_item"]
            if hasattr(first_source_item, 'file_ext') and first_source_item.file_ext:
                file_ext = first_source_item.file_ext
                if isinstance(file_ext, dict):
                    # åªä¼ é€’å…¬å…±å‚æ•°ï¼Œä¸ä¼ é€’ç‰¹å®šæ–‡ä»¶çš„å‚æ•°
                    common_params = {k: v for k, v in file_ext.items() 
                                   if k not in ['share_fid_token']}
                    current_transfer_ext_params.update(common_params)
                    
                    # æ·»åŠ åˆ†äº«æ–‡ä»¶çš„çˆ¶ç›®å½•ID
                    if first_source_item.parent_id:
                        current_transfer_ext_params['share_parent_fid'] = first_source_item.parent_id
            
            # å¦‚æœsource_definitionæœ‰ext_paramsï¼Œä¹Ÿä¸€å¹¶åŠ å…¥
            if hasattr(source_definition, 'ext_params') and source_definition.ext_params:
                if isinstance(source_definition.ext_params, dict):
                    # å°†source_definition.ext_paramsä¸­çš„æ‰€æœ‰å‚æ•°åˆå¹¶åˆ°current_transfer_ext_params
                    current_transfer_ext_params.update(source_definition.ext_params)
            
            # è·å–ç›®æ ‡ç›®å½•çš„file_idï¼ˆä»æ¯”è¾ƒç»“æœä¸­è·å–ï¼‰
            target_dir_file_id = None
            
            # 1. ä¼˜å…ˆä½¿ç”¨æ¯”è¾ƒç»“æœä¸­çš„file_id
            if add_items_in_group:
                target_dir_file_id = add_items_in_group[0].get("target_parent_file_id")
            
            # 2. å¦‚æœæ˜¯æ ¹ç›®å½•ï¼Œä½¿ç”¨target_definitionä¸­çš„file_id
            if not target_dir_file_id and normalized_target_parent_dir == target_definition.file_path:
                target_dir_file_id = target_definition.file_id
            
            if not target_dir_file_id:
                error_msg = f"æ— æ³•è·å–ç›®æ ‡ç›®å½•çš„file_id: {normalized_target_parent_dir}"
                for add_item in add_items_in_group:
                    source_item = add_item["source_item"]
                    target_path = add_item["target_full_path"]
                    operation_results['failed'].append(f"TRANSFER_ERROR: {source_item.file_path} -> {target_path} - {error_msg}")
                continue
            
            # æ„å»ºtransferæ‰€éœ€å‚æ•°
            try:
                # æ„å»º TransferParam
                transfer_params = TransferParam(
                    drive_type=drive_type_str,
                    source_type=source_definition.source_type,
                    source_id=source_definition.source_id,
                    source_path=source_definition.file_path,
                    target_path=normalized_target_parent_dir,
                    target_id=target_dir_file_id,  # ä½¿ç”¨è·å–åˆ°çš„å…·ä½“ç›®å½•file_id
                    file_ids=source_fs_ids_to_transfer,
                    ext=current_transfer_ext_params
                )
                
                # ä½¿ç”¨ç»Ÿä¸€æ¶æ„çš„transferæ–¹æ³•
                transfer_success = await drive_manager.transfer_files(x_token, transfer_params)
                
                if transfer_success:
                    # è½¬å­˜æˆåŠŸï¼Œè®°å½•æ‰€æœ‰æ–‡ä»¶ä¸ºæˆåŠŸ
                    for add_item in add_items_in_group:
                        source_item = add_item["source_item"]
                        target_path = add_item["target_full_path"]
                        operation_results['succeeded'].append(f"TRANSFER_SUCCESS: {source_item.file_path} -> {target_path}")
                else:
                    # è½¬å­˜å¤±è´¥ï¼Œè®°å½•æ‰€æœ‰æ–‡ä»¶ä¸ºå¤±è´¥
                    for add_item in add_items_in_group:
                        source_item = add_item["source_item"]
                        target_path = add_item["target_full_path"]
                        operation_results['failed'].append(f"TRANSFER_FAIL: {source_item.file_path} -> {target_path}")
            except Exception as ex_transfer:
                # è®°å½•æ‰€æœ‰æ–‡ä»¶ä¼ è¾“å¤±è´¥
                for add_item in add_items_in_group:
                    source_item = add_item["source_item"]
                    target_path = add_item["target_full_path"]
                    operation_results['failed'].append(f"TRANSFER_ERROR: {source_item.file_path} -> {target_path} - {str(ex_transfer)}")
        except Exception as ex_group:
            # æ•´ç»„å¤„ç†å‡ºé”™
            for add_item in add_items_in_group:
                source_item = add_item["source_item"]
                target_path = add_item["target_full_path"]
                operation_results['failed'].append(f"TRANSFER_GROUP_ERROR: {source_item.file_path} -> {target_path} - {str(ex_group)}")
    
    return operation_results

async def _process_delete_operations(
    drive_manager: Any,
    x_token: str,
    to_delete: List[BaseFileInfo],
    drive_type_str: str
) -> Dict[str, List[str]]:
    """
    å¤„ç†åˆ é™¤æ“ä½œ
    
    :param drive_manager: ç½‘ç›˜ç®¡ç†å™¨å®ä¾‹
    :param x_token: è®¤è¯ä»¤ç‰Œ
    :param to_delete: è¦åˆ é™¤çš„æ–‡ä»¶/ç›®å½•åˆ—è¡¨
    :param drive_type_str: ç½‘ç›˜ç±»å‹å­—ç¬¦ä¸²
    :return: æ“ä½œç»“æœï¼ŒåŒ…å«succeededå’Œfailedä¸¤ä¸ªåˆ—è¡¨
    """
    operation_results = {'succeeded': [], 'failed': []}
    
    # æ”¶é›†æ‰€æœ‰è¦åˆ é™¤çš„æ–‡ä»¶è·¯å¾„å’ŒID
    file_paths = []
    file_ids = []
    
    for item in to_delete:
        if item.file_path:
            # ç¡®ä¿è·¯å¾„æ˜¯ä»¥/å¼€å¤´çš„ç»å¯¹è·¯å¾„
            path = item.file_path
            if not path.startswith("/"):
                path = "/" + path
            file_paths.append(path)
        if item.file_id:
            file_ids.append(item.file_id)
    
    # æ„å»º RemoveParam
    try:
        remove_params = RemoveParam(
            drive_type=drive_type_str,
            file_paths=file_paths,
            file_ids=file_ids
        )
        
        result = await drive_manager.remove_files(x_token, remove_params)
        if result:
            for item in to_delete:
                operation_results['succeeded'].append(f"DELETE_SUCCESS: {item.file_path} (ID: {item.file_id})")
        else:
            for item in to_delete:
                operation_results['failed'].append(f"DELETE_FAILED: {item.file_path} (ID: {item.file_id})")
    except Exception as e:
        for item in to_delete:
            operation_results['failed'].append(f"DELETE_ERROR: {item.file_path} (ID: {item.file_id}) - {str(e)}")
    
    return operation_results 